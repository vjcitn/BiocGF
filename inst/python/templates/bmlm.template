# run with BiocGF basilisk env
from transformers import AutoTokenizer, BertForMaskedLM, logging
import torch

logging.set_verbosity_error()

#tokenizer = AutoTokenizer.from_pretrained("google-bert/bert-base-uncased")
#model = BertForMaskedLM.from_pretrained("google-bert/bert-base-uncased")
#
# following downloads 1.3 GB of weights
#
tokenizer = AutoTokenizer.from_pretrained("google-bert/bert-large-cased")
model = BertForMaskedLM.from_pretrained("google-bert/bert-large-cased")

inputs = tokenizer("%%ONEMASKPHRASE%%", return_tensors="pt")

with torch.no_grad(): 
    logits = model(**inputs).logits

# retrieve index of [MASK]
mask_token_index = (inputs.input_ids == tokenizer.mask_token_id)[0].nonzero(as_tuple=True)[0]

predicted_token_id = logits[0, mask_token_index].argmax(axis=-1)
tokenizer.decode(predicted_token_id)  # so tokenizer instance is returned
